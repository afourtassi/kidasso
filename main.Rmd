---
title: "conceptNet"
date: "2/15/2018"
output: html_document
---

Libraries
```{r setup, include=FALSE}
 library(purrr)
  library(readr)
  library(ggplot2)
  library(langcog)
  library(boot)
  library(dplyr)
  library(tidyr)
  library(wordbankr)
  library(directlabels)
  library(stringr)
  library(lmtest)
  library(rwebppl)
  library(jsonlite)
  library(nlme)
  library(feather)
  library(broom)
  library(HDInterval)
  library(BBmisc)
  library(wordVectors)
  library(magrittr)
```

Data

```{r}

d <- read_delim("data/associations.txt", delim = "\t") %>%
  mutate(Age3 = ifelse(Age2 == 'Older' | Age2 == 'Younger' , 'Young', 'Adult')) 
#Throughout the analyses I collapse Younger and Older when there is no difference between their measures

dict <- read_delim("data/cmu_dict.txt", delim = ",") 

```

example

```{r}

 g6b_300 <- scan(file = "derived/glove.6B/glove.6B.300d.txt", what="", sep="\n")

```

```{r}

###Function  written by Taylor Van Anne
proc_pretrained_vec <- function(p_vec) {


        # initialize space for values and the names of each word in vocab
        vals <- vector(mode = "list", length(p_vec))
        names <- character(length(p_vec))

        # loop through to gather values and names of each word
        for(i in 1:length(p_vec)) {
            if(i %% 1000 == 0) {print(i)}
            this_vec <- p_vec[i]
            this_vec_unlisted <- unlist(strsplit(this_vec, " "))
            this_vec_values <- as.numeric(this_vec_unlisted[-1])  # this needs testing, does it become numeric?
            this_vec_name <- this_vec_unlisted[1]

            vals[[i]] <- this_vec_values
            names[[i]] <- this_vec_name
        }

        # convert lists to data.frame and attach the names
        glove <- data.frame(vals)
        names(glove) <- names

        return(glove)
}


glove.300 <- proc_pretrained_vec(g6b_300)

 find_sim_wvs <- function(this_wv, all_wvs, top_n_res=40) {
        # this_wv will be a numeric vector; all_wvs will be a data.frame with words as columns and dimesions as rows
        require(text2vec)
        this_wv_mat <- matrix(this_wv, ncol=length(this_wv), nrow=1)
        all_wvs_mat <- as.matrix(all_wvs)
        
        if(dim(this_wv_mat)[[2]] != dim(all_wvs_mat)[[2]]) {
            print("switching dimensions on the all_wvs_matrix")
            all_wvs_mat <- t(all_wvs_mat)
        }
        
        cos_sim = sim2(x=all_wvs_mat, y=this_wv_mat, method="cosine", norm="l2")
        sorted_cos_sim <- sort(cos_sim[,1], decreasing = T) 
        return(head(sorted_cos_sim, top_n_res))
                
    }

 this_word_vector <- glove.300[['king']] - glove.300[['man']] + glove.300[['woman']] 
 find_sim_wvs(glove.300[['king']], glove.300, top_n_res=5)
 
 
 example <- t(glove.300)
 #Select from the matrix only the words that exist in our database
 
```




This function is used to recode symbols in cmu phonetic dictionary: we want each symbol to be unique character ( the right format for the Edit distance) 
```{r}

#Read in phones and the corresponding set of characters

characters <- read_delim("data/symbolsToChar.txt", delim = "\t", col_names = FALSE) 
phones <- read_delim("data/cmu_symbols.txt", delim = "\t", col_names = FALSE)
phon_dict <- bind_cols(phones, characters)

#Converts string to its representation using the symbols
convert <- function(str, combined) {
    if (typeof(str) != "character" && class(str) != "factor") 
        stop(sprintf("Illegal data type: %s", typeof(str)))
    if (class(str) == "factor") 
        str = as.character(str)
    if (length(str) == 0)
        return(integer(0))
    splitstring = strsplit(str, split=" ")
    result_string = ""
    for (i in splitstring) {
      for (j in i) {
        filtered <- filter(combined, X1 == j)
        converted <- select(filtered, X11)
        result_string <- paste(result_string, converted)
        result_string <- gsub("[[:space:]]", "", result_string)
      }
    }
    return(result_string)
}

```
First, do some summary statistics 
```{r}
#Number of participants by age group
n_subjects <- d %>%
  group_by(Age2, Subject) %>%
  summarise(n_words=n()) %>%
  group_by(Age2) %>%
  summarise(n=n())

#Number of words by age group
n_words <- d %>%
  group_by(Age2, Experimenter_Word) %>%
  summarise(n_subj=n()) %>%
  group_by(Age2) %>%
  summarise(n=n(),
            ave = mean(n_subj))

#Age groups

age_groups <- d %>%
  distinct(Age2, Age)

#Younger -> 3, 4, 5
#Older -> 6, 7, 8
#Adults -> 18, 19, 20, 21, 22, 24, 26, 38, 43

  
```


Reproduce the development of paradigmatic vs. syntagmatic as in Erica's previous research

```{r}
d_paradig <- d %>%
  group_by(Age2) %>%
  summarise(mean = mean(Para),
            sd = sd(Para),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)
  
d_paradig$Age2 = factor(d_paradig$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(d_paradig, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) 

```

Entropy (degree of agreement on responses among subjects) of answers as a function af age (high entropy -> low agreement)
Prediction: agreement between participants increases as a function of age 

```{r}

#Definition of the entropy
#-sum(p*log_2(p))

#Normalized entropy (what we use here)
#-sum(p*log_2(p))/log_2(n_sample)

d_entropy <- d %>%
  group_by(Experimenter_Word, Age3, Child_Word) %>%
  summarise(n=n()) %>%
  mutate(p = n/sum(n)) %>%
  group_by(Experimenter_Word, Age3) %>%
  summarise(entropy = -sum(p*log2(p)), 
            total = n()) %>%
  mutate(entropy_norm = entropy/log2(total)) %>%# Normalized entropy
  filter(!is.na(entropy_norm)) %>%
  group_by(Age3) %>%
  summarise(mean = mean(entropy_norm), 
            sd = sd(entropy_norm),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)

d_entropy$Age3 = factor(d_entropy$Age3, levels = c("Young", "Adult")) 
#d_entropy$Age2 = factor(d_entropy$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(d_entropy, aes(x=Age3, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) #+facet_grid( . ~ partofspeech) 


#Breaking by part of speech gives similar results within lexical category

# Do some statistical tests?

```

Include the phonetic distance in the datafarame 

```{r}
#Associations have the form: Cue -> Target

#Phonetic transcripton for Cues
Cues <- d %>%
  select(Experimenter_Word) %>%
  distinct() %>%
  mutate(Word = toupper(Experimenter_Word)) %>%
  left_join(dict) %>%
  select(-Word) %>%
  rename(cue_phon = Phonetic)

#Phonetic transcripton for Targets
Targets <- d %>%
  select(Child_Word) %>%
  distinct() %>%
  mutate(Word = toupper(Child_Word)) %>%
  left_join(dict) %>%
  select(-Word) %>%
  rename(target_phon = Phonetic)

#Include in the originial dataset
d_phon <- d %>%
  left_join(Cues) %>%
  left_join(Targets) %>%
  rowwise() %>% 
  mutate( #herer we recode the phonetic transcription to be able to use it with edit distance built in R 
    cue_code = convert(cue_phon, phon_dict), 
    target_code = ifelse(!is.na(target_phon), convert(target_phon, phon_dict), NA) #make an if statement
    ) %>%
  mutate(phon_dist =  ifelse(!is.na(target_code), adist(cue_code, target_code), NA)) 


```

Include semantic distance from Childes to the dataframe
```{r}
#Generate the model and store it in "derived"

##Uncomment the following code to re-run the Word2Vec model

#model = train_word2vec("data/corpus.txt",
#                            output="corpus.bin", threads = 4,
#                             vectors = 100, window=20, cbow=1, min_count = 10, force= TRUE)

#Read the model

model_cds = read.vectors("derived/corpus.bin")

#model_google = read.vectors("derived/freebase.bin")

model <- as.VectorSpaceModel(example)

cue_words <- (d %>%
  distinct(Experimenter_Word))$Experimenter_Word

target_words <- (d %>%
  distinct(Child_Word))$Child_Word

#Glove vectors
model_cue <- model[[which(rownames(model) %in% cue_words), average=FALSE]]
model_target <- model[[which(rownames(model) %in% target_words), average=FALSE]]
cosSim <- cosineSimilarity(model_cue, model_target)

pairs <- na.omit(data.frame(as.table(cosSim))) %>%
  rename(Experimenter_Word=Var1,
         Child_Word=Var2,
         CosSim=Freq)

#feather::write_feather(pairs, "derived/CosSim_glov.feather")

pairs <- feather::read_feather("derived/CosSim_glov.feather")

#CDS
model_cue_cds <- model_cds[[which(rownames(model_cds) %in% cue_words), average=FALSE]]
model_target_cds <- model_cds[[which(rownames(model_cds) %in% target_words), average=FALSE]]
cosSim_cds <- cosineSimilarity(model_cue_cds, model_target_cds)

pairs_cds <- na.omit(data.frame(as.table(cosSim_cds))) %>%
  rename(Experimenter_Word=Var1,
         Child_Word=Var2,
         CosSim_cds=Freq)

#feather::write_feather(pairs_cds, "derived/CosSim_cds.feather")

pairs_cds <- feather::read_feather("derived/CosSim_cds.feather")


d_phon_sem <- d_phon %>%
  left_join(pairs) %>%
  left_join(pairs_cds)

#Basline (chance) semantic distance
pair_chance <- pairs %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)

#Baseline (chance) semantic distance CDS
pair_chance_cds <- pairs_cds %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)



#Here select the variables that are useful to later analyses

d_all <- d_phon_sem %>%
  select(Subject, partofspeech, Experimenter_Word, Child_Word,  Age, Age2, Age3, cue_phon, target_phon,  cue_code, target_code, phon_dist, CosSim, CosSim_cds) %>%
  dplyr::rename(cue = Experimenter_Word, 
                target = Child_Word) 

```

How does phonetic distance predict associations?
Prediciton: children will use shorter words 
```{r}

#compute average of phondistan
#Model predcit phonDist as a function of age (fixed) and Subj + cur as (ransom) will that converge?

PhonDist <- d_all %>%
  filter(!is.na(phon_dist),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         ) %>%
  group_by(Age3) %>%
  summarise(mean = mean(phon_dist),
            sd = sd(phon_dist),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)
  

PhonDist$Age3 = factor(PhonDist$Age3, levels = c("Young", "Adult"))
#PhonDist$Age2 = factor(PhonDist$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(PhonDist, aes(x=Age3, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1))



```

Do the same with the semantic distance
Predcition: semantic similarity will increase as a function of age 
```{r}

SemDist <- d_all %>%
  filter(!is.na(CosSim),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         ) %>%
  group_by(Age3) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 
  

SemDist$Age3 = factor(SemDist$Age3, levels = c("Young", "Adult"))
#SemDist$Age2 = factor(SemDist$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(SemDist, aes(x=Age3, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) #+ facet_grid( . ~ partofspeech) 

```

More analyses on the phonetic distance 
why do children use shorter words? The explanation is that they use more minimal pairs (because phonetic similarity plays an role in the lexical organization)
Indeed we find that, for each cue, the probability that this cue will be given a minimla par as a target is disproportinately higher for children, whereas the probability that it will be given a phonetically different target is not very different betwenn the two age groups
```{r}

#Historams
Phon_histo <- d_all %>%
   filter(!is.na(phon_dist),
         phon_dist != 0
         ) %>%
  group_by(Age3, phon_dist) %>%
  summarise(n0 = n()) %>%
  mutate(p = n0/sum(n0)) 

ggplot(Phon_histo, aes(x=phon_dist, y=p))+
  geom_col()+
  facet_grid(. ~ Age3)


#How to quantify the rate of minimal pairs?
#E.g., for each cue, what's the probability that the target will be a minimal pair
#This means, for a given cue, what is the percentage of subjets of gave a minimal pair

miniDist <- d_all %>%
  filter(!is.na(phon_dist),
         phon_dist != 0 
         ) %>%
  group_by(Age3, cue) %>% #Get a measure for each word, across subjects
  summarise(n = n(),
            dist1 = sum(phon_dist==1)/n(),
            dist2 = sum(phon_dist==2)/n(),
            dist3 = sum(phon_dist==3)/n(), 
            dist4 = sum(phon_dist==4)/n(),
            dist5 = sum(phon_dist==5)/n(), 
            dist6 = sum(phon_dist==6)/n(),
            dist7 = sum(phon_dist==7)/n(), 
            dist8 = sum(phon_dist==8)/n(),
            dist9 = sum(phon_dist==9)/n(), 
            dist10 = sum(phon_dist==10)/n(),
            dist11 = sum(phon_dist==11)/n()
            ) %>%
  gather(phon_dist, prob, dist1:dist11) %>%
  group_by(Age3, phon_dist) %>%
  summarise(mean = mean(prob),
            sd = sd(prob),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)
  
miniDist$phon_dist = factor(miniDist$phon_dist, levels = c("dist1", "dist2", "dist3", "dist4", "dist5", "dist6", "dist7", "dist8", "dist9", "dist10", "dist11"))

ggplot(miniDist, aes(x=phon_dist, y=mean, fill=Age3))+
  geom_line(aes(x=phon_dist, y=mean))+
  geom_col(position = "dodge")+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 1), size=0.3) 


```


Density plot for the semantic distance
It turns out many words are missing from word2vec data, we need to use more comprehensive vectors (google glov?)

```{r}

Sem_histo <- d_all %>%
   filter(!is.na(CosSim),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         )

ggplot(Sem_histo,  aes(CosSim, fill=Age3))+geom_density(aes(y=..scaled..), alpha=0.2, adjust= 1.5) + theme(aspect.ratio = 0.7, axis.text=element_text(size=7, angle = 45)) +xlab("Cosine similarity") +ylab("Count")


```

Correlation between phonetic and semantic distance 
Here again we have substantially less data because of the missing data from semantic distance. We need Glov vectors to move forward 
```{r}
##Correlation semantic phonology
phon_sem <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0 
         )

ggplot(phon_sem, aes(x=phon_dist, y=CosSim))+
#ggplot(phon_sem_dist, aes(x=Phon_dist, y=CosSim))+
  geom_point()+
  geom_smooth(method = "lm")+
  #geom_smooth()+
  facet_grid(. ~ Age3)

#Correlatin test Adults (negative correlation)
adult <- subset(phon_sem, Age3=='Adult')
cor.test(adult$phon_dist, adult$CosSim)

#Correlatin test Children (positive correlation!!!)
children <- subset(phon_sem, Age3=='Young')
cor.test(children$phon_dist, children$CosSim)

```



```{r}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 
phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  group_by(Age3,phon_dist) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))#+
  #facet_grid(. ~ Age3)


```

```{r}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 
phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim_cds),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  group_by(Age3,phon_dist) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))#+
  #facet_grid(. ~ Age3)


```

```{r}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 
phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  mutate(minimal = ifelse(phon_dist==1,"Minimal","noMinial")) %>%
  group_by(Age3, minimal) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=minimal, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))+
  facet_grid(. ~ Age3)


```
```{r}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 

phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim_cds),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  mutate(minimal = ifelse(phon_dist==1,"Minimal","noMinial")) %>%
  group_by(Age3, minimal) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=minimal, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))+
  facet_grid(. ~ Age3)


```

