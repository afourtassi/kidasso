---
title: "Free Associations in Children and Adults"
date: "5/04/2018"
output: html_document
---


```{r setup, include=FALSE, echo=FALSE}
 library(purrr)
  library(readr)
  library(ggplot2)
  library(langcog)
  library(boot)
  library(dplyr)
  library(tidyr)
  library(wordbankr)
  library(directlabels)
  library(stringr)
  library(lmtest)
  library(rwebppl)
  library(jsonlite)
  library(nlme)
  library(feather)
  library(broom)
  library(HDInterval)
  library(BBmisc)
  library(wordVectors)
  library(magrittr)
  library(lme4)
```


```{r include=FALSE, echo=FALSE}

d <- read_delim("data/associations.txt", delim = "\t") %>%
  mutate(Age3 = ifelse(Age2 == 'Older' | Age2 == 'Younger' , 'Young', 'Adult')) 
#Throughout the analyses I collapse Younger and Older when there is no difference between their measures

dict <- read_delim("data/cmu_dict.txt", delim = ",") 

```


```{r, include=FALSE, echo=FALSE}

#Uncomment if you want to de the prprocessing instead of using the pre-processed data stored in "derived"

 #g6b_300 <- scan(file = "derived/glove.6B/glove.6B.300d.txt", what="", sep="\n")

```


```{r, include=FALSE, echo=FALSE}

###Function to prepocess the Glove vectos  (by Taylor Van Anne)
proc_pretrained_vec <- function(p_vec) {


        # initialize space for values and the names of each word in vocab
        vals <- vector(mode = "list", length(p_vec))
        names <- character(length(p_vec))

        # loop through to gather values and names of each word
        for(i in 1:length(p_vec)) {
            if(i %% 1000 == 0) {print(i)}
            this_vec <- p_vec[i]
            this_vec_unlisted <- unlist(strsplit(this_vec, " "))
            this_vec_values <- as.numeric(this_vec_unlisted[-1])  # this needs testing, does it become numeric?
            this_vec_name <- this_vec_unlisted[1]

            vals[[i]] <- this_vec_values
            names[[i]] <- this_vec_name
        }

        # convert lists to data.frame and attach the names
        glove <- data.frame(vals)
        names(glove) <- names

        return(glove)
}


#glove.300 <- proc_pretrained_vec(g6b_300)

#convert to a  more appropriate foramt
#glove.matrix <- t(glove.300)

 
```

```{r include=FALSE}
#This function is used to recode symbols in cmu phonetic dictionary: we want each symbol to be unique character. This is the right input for the Edit distance function (written by Megumi Sano)


characters <- read_delim("data/symbolsToChar.txt", delim = "\t", col_names = FALSE) 
phones <- read_delim("data/cmu_symbols.txt", delim = "\t", col_names = FALSE)
phon_dict <- bind_cols(phones, characters)

#Converts string to its representation using the symbols
convert <- function(str, combined) {
    if (typeof(str) != "character" && class(str) != "factor") 
        stop(sprintf("Illegal data type: %s", typeof(str)))
    if (class(str) == "factor") 
        str = as.character(str)
    if (length(str) == 0)
        return(integer(0))
    splitstring = strsplit(str, split=" ")
    result_string = ""
    for (i in splitstring) {
      for (j in i) {
        filtered <- filter(combined, X1 == j)
        converted <- select(filtered, X11)
        result_string <- paste(result_string, converted)
        result_string <- gsub("[[:space:]]", "", result_string)
      }
    }
    return(result_string)
}

```
#### Main findings 

-There is high individual variability in children's lexical organization, compared to adults' organization.  

-Despite this high individual variability, the associations produced by children have interesting common properties:

1) They tend to be more phonetically similar. In particular, children tend to have more minimal pairs in their associations (e.g., "house" -> "mouse").

2) They tend to be less thematically/semantically related, although not at chance. 

3) The phonetic and semantic relatedness in children's associations interact: The pairs that are more similar phonetically are less similar semantically.  



#### Descriptive statistics 

Number of subjects in each age group (I, sometimes, collapse "Younger" and "Older" in one category to have more power).
```{r echo=FALSE}
#Number of participants by age group
n_subjects <- d %>%
  group_by(Age2, Subject) %>%
  summarise(n_words=n()) %>%
  group_by(Age2) %>%
  summarise(N_subjects=n())

n_subjects
```

Number of cues (Experimenter's word) by age group, and average number of subjects per cue
```{r echo=FALSE}
#Number of words by age group, and average subjects by word
n_words <- d %>%
  group_by(Age2, Experimenter_Word) %>%
  summarise(n_subj=n()) %>%
  group_by(Age2) %>%
  summarise(N_cues=n(),
            Ave_subjects = mean(n_subj))

n_words
```
Ages in each  group
```{r echo=FALSE}

#Age groups
age_groups <- d %>%
  distinct(Age2, Age)

age_groups[order(age_groups$Age),]

  
```



#### Result 0: Development of paradigmatic vs. syntagmatic associations 

This is a replication of previous findings (Cite XX). The relations were hand-coded into paradigmatic or syntagmatic.
```{r echo=FALSE}
d_paradig <- d %>%
  group_by(Age2) %>%
  summarise(mean = mean(Para),
            sd = sd(Para),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)
  
d_paradig$Age2 = factor(d_paradig$Age2, levels = c("Younger", "Older", "Adult"))


ggplot(d_paradig, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) +
  ylab('mean praadigmatic answers')

```




#### Result 1: Development of associations' entropy 

I measure agreement among subjects in their associations as a function of age. Agreement is quantified through standard information-theoretical measures (here I use Normalized Entropy).

For each cue $y$ (i.e., Experimenter's word), I compute the normalized entropy defined as: 

$$H(y)=\sum_{i=1}^{N}   \frac{p(x_i)*log_2(p(x_i))}{log_2(N)} $$
$p(x_i)$ is the probability of a target $x_i$ (i.e., Child's word), which I obtain, for each cue, through averaging across subjects' responses for that cue. $H$ have values between 0 (no disagreement) and 1 (no agreement).

$N$ is the total number of answers provided by all subjects for a given cue.

The graph, below, shows the average entropy across all cues (with 95% CI). 


```{r echo=FALSE}

#Definition of the entropy
#-sum(p*log_2(p))

#Normalized entropy (what we use here)
#-sum(p*log_2(p))/log_2(n_sample)


#Here I need a way to keep the distribution and add mean and CI only in the plot, so as to  
d_entropy <- d %>%
  group_by(Experimenter_Word, Age2, Child_Word) %>%
  summarise(n=n()) %>%
  mutate(p = n/sum(n)) %>%
  group_by(Experimenter_Word, Age2) %>%
  summarise(entropy = -sum(p*log2(p)), 
            total = n()) %>%
  mutate(entropy_norm = entropy/log2(total)) %>%# Normalized entropy
  filter(!is.na(entropy_norm)) %>%
  group_by(Age2) %>%
  summarise(mean = mean(entropy_norm), 
            sd = sd(entropy_norm),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)

#d_entropy$Age3 = factor(d_entropy$Age3, levels = c("Young", "Adult")) 
d_entropy$Age2 = factor(d_entropy$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(d_entropy, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1))+
  ylab('mean normalized entropy')
  #+facet_grid( . ~ partofspeech)

# Do some statistical tests?
#We comapre: how the entropy is distributed for each cue across age groups
##Model predicts entropy (response) by Age agroup (predictor)


#fit_entrop <- lmer(entropy_norm ~ Age3 + (1|Experimenter_Word), data= d_entropy)
#confint(fit_entrop)
```

I find that entropy for children is very high, and it is higher than adults' entropy. This means that, compared to adults, children differ strongly at the individual level in terms of their associations.

Below I study the properties of the cue -> target relations. I investigate the phonetic and semantic proximity, as well as the correlation between these two measures.



#### Result 2: Development of the phonetic proximity in the free associations

I investigate the extent to which the cue -> target relations are determined by phonetic proximity (e.g., "house" -> "mouse").

I convert the orthographic words into their phonetic transcription, and I measure the Levenshtein distance (also known as "edit" distance). This measure counts the minimum number of operations (insertions, deletions, substitutions) required to change one string into another. For example, the edit distance of the pair (house -> mouse) is 1, because we only needed 1 operation: substituting "h" with "m" (for illustration this example was given with the orthographic transcription).

I remove from the analysis the case where targerts were identical to the cues (e.g., cat -> cat)

```{r echo=FALSE, message=FALSE}
#Associations have the form: Cue -> Target

#Here we compute the phonetic proximity between the cue and the target 

#Phonetic transcripton for Cues
Cues <- d %>%
  select(Experimenter_Word) %>%
  distinct() %>%
  mutate(Word = toupper(Experimenter_Word)) %>%
  left_join(dict) %>%
  select(-Word) %>%
  rename(cue_phon = Phonetic)

#Phonetic transcripton for Targets
Targets <- d %>%
  select(Child_Word) %>%
  distinct() %>%
  mutate(Word = toupper(Child_Word)) %>%
  left_join(dict) %>%
  select(-Word) %>%
  rename(target_phon = Phonetic)

#Here Uncomment to do the 
#Include in the originial dataset
#d_phon <- d %>%
#  left_join(Cues) %>%
#  left_join(Targets) %>%
#  rowwise() %>% 
#  mutate( #herer we recode the phonetic transcription to be able to use it with edit distance built in R 
#       cue_code = convert(cue_phon, phon_dict), 
#    target_code = ifelse(!is.na(target_phon), convert(target_phon, phon_dict), NA) #make an if statement
#    ) %>%
#  mutate(phon_dist =  ifelse(!is.na(target_code), adist(cue_code, target_code), NA)) 
#
#feather::write_feather(d_phon, "derived/d_phon.feather")

d_phon <- feather::read_feather("derived/d_phon.feather") %>%
  dplyr::rename(cue = Experimenter_Word, 
                target = Child_Word) 

```

```{r echo=FALSE, , message=FALSE}

#compute average of phon distance

PhonDist <- d_phon %>%
  filter(!is.na(phon_dist),
         phon_dist != 0 #We  eliminate words where children just repeat the cue (e.g., apple -> apple)
         ) %>%
  group_by(Age2) %>%
  summarise(mean = mean(phon_dist),
            sd = sd(phon_dist),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)
  

#PhonDist$Age3 = factor(PhonDist$Age3, levels = c("Young", "Adult"))
PhonDist$Age2 = factor(PhonDist$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(PhonDist, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) +
  ylab('mean Phonetic edit distance')

#fit_phon <- lmer(phon_dist ~ Age3 + (Age3|cue) + (1|Subject) , data= PhonDist)

#confint(fit_phon)

```

I found that the children's associations are, on average, more phonetically related than adults' associations. The interesting explanation is that children's free associations are more determined by their phonetic proximity. But if this is the case, we would expect the difference between children and adults to be driven mostly by minimal pairs (e.g., mouse -> house). We test this prediction in what follows.

For each cue, I measure the probability that the target will have a given edit distance. E.g., for the cue "house", what is the probability that the target will have an edit distance of 1 (e.g., "mouse"), an edit distance of 2 (e.g., "mouth"),....Then I compute the average proportions ocross all cues (with 95% CI). 

I collapse "Older" and "Younger"" into one Age group ("Young").

```{r echo=FALSE, message=FALSE}

#Historams
Phon_histo <- d_phon %>%
   filter(!is.na(phon_dist),
         phon_dist != 0
         ) %>%
  group_by(Age3, phon_dist) %>%
  summarise(n0 = n()) %>%
  mutate(p = n0/sum(n0)) 

#ggplot(Phon_histo, aes(x=phon_dist, y=p))+
#  geom_col()+
#  facet_grid(. ~ Age3)


#How to quantify the rate of minimal pairs?
#E.g., for each cue, what's the probability that the target will be a minimal pair
#This means, for a given cue, what is the percentage of subjets of gave a minimal pair

miniDist <- d_phon %>%
  filter(!is.na(phon_dist),
         phon_dist != 0 
         ) %>%
  group_by(Age3, cue) %>% #Get a measure for each word, across subjects
  summarise(n = n(),
            dist1 = sum(phon_dist==1)/n(),
            dist2 = sum(phon_dist==2)/n(),
            dist3 = sum(phon_dist==3)/n(), 
            dist4 = sum(phon_dist==4)/n(),
            dist5 = sum(phon_dist==5)/n(), 
            dist6 = sum(phon_dist==6)/n(),
            dist7 = sum(phon_dist==7)/n(), 
            dist8 = sum(phon_dist==8)/n(),
            dist9 = sum(phon_dist==9)/n(), 
            dist10 = sum(phon_dist==10)/n(),
            dist11 = sum(phon_dist==11)/n()
            ) %>%
  gather(phon_dist, prob, dist1:dist11) %>%
  group_by(Age3, phon_dist) %>%
  summarise(mean = mean(prob),
            sd = sd(prob),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)
  
miniDist$phon_dist = factor(miniDist$phon_dist, levels = c("dist1", "dist2", "dist3", "dist4", "dist5", "dist6", "dist7", "dist8", "dist9", "dist10", "dist11"))

ggplot(miniDist, aes(x=phon_dist, y=mean, fill=Age3))+
  geom_line(aes(x=phon_dist, y=mean))+
  geom_col(position = "dodge")+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 1), size=0.3) 


```

For a given cue, children and adults are equally likely to give a target which is NOT a minimal pair. They only differ in the probability of answering with a minimal pair: Children are more likely to give a minimal pair as a target.


#### Result 3: Development of the semantic proximity in the free associations

Now I investigate the semantic proximity of the free associations. 

I measure the semantic similarity of the cue-target pairs using the state-of-the-art distributional semantic model known as Word2Vec. 
In this model, two words are similar if they co-occur in a large corpus of text. 

I use vectors trained on two kinds of texts:

**Measure 1:** the semantic similarity is derived from a model trained on a (very) large Wikipedia corpus (around 6B tokens). This measure is supposed to offer a quantification of similarity from an adult perspective. It is supposed to be a rather "objective" representation of semantic similarity in English. 

**Measure 2:** the semantic similarity is derived from a model trained on a corpus of child directed speech. This measure is supposed to approximate the perspective of the child. This is because children are more likely to derive co-occurrence similarity from such specific corpus, rather than from a more language-representative text such as wikipedia.  


```{r include=FALSE}
#Uncomment the following to preprocess the semantic distance 

##Uncomment the following code to re-run the Word2Vec model

#model = train_word2vec("data/corpus.txt",
#                            output="corpus.bin", threads = 4,
#                             vectors = 100, window=20, cbow=1, min_count = 10, force= TRUE)

#Read the model

#model_cds = read.vectors("derived/corpus.bin")

#model_google = read.vectors("derived/freebase.bin")

#model <- as.VectorSpaceModel(example)
#
#cue_words <- (d %>%
#  distinct(Experimenter_Word))$Experimenter_Word

#target_words <- (d %>%
#  distinct(Child_Word))$Child_Word

#Glove vectors
#model_cue <- model[[which(rownames(model) %in% cue_words), average=FALSE]]
#model_target <- model[[which(rownames(model) %in% target_words), average=FALSE]]
#cosSim <- cosineSimilarity(model_cue, model_target)

#pairs <- na.omit(data.frame(as.table(cosSim))) %>%
#  rename(Experimenter_Word=Var1,
#         Child_Word=Var2,
#         CosSim=Freq)

#feather::write_feather(pairs, "derived/CosSim_glov.feather")

pairs <- feather::read_feather("derived/CosSim_glov.feather")%>%
  dplyr::rename(cue = Experimenter_Word, 
                target = Child_Word) 

#CDS
#model_cue_cds <- model_cds[[which(rownames(model_cds) %in% cue_words), average=FALSE]]
#model_target_cds <- model_cds[[which(rownames(model_cds) %in% target_words), average=FALSE]]
#cosSim_cds <- cosineSimilarity(model_cue_cds, model_target_cds)

#pairs_cds <- na.omit(data.frame(as.table(cosSim_cds))) %>%
#  rename(Experimenter_Word=Var1,
#         Child_Word=Var2,
 #        CosSim_cds=Freq)

#feather::write_feather(pairs_cds, "derived/CosSim_cds.feather")

pairs_cds <- feather::read_feather("derived/CosSim_cds.feather") %>%
  dplyr::rename(cue = Experimenter_Word, 
                target = Child_Word) 


d_phon_sem <- d_phon %>%
  left_join(pairs) %>%
  left_join(pairs_cds)

#Basline (chance) in the semantic distance
pair_chance <- pairs %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)

#Baseline (chance) semantic distance CDS
pair_chance_cds <- pairs_cds %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se)



#Here select the variables that are useful to later analyses

d_all <- d_phon_sem %>%
  select(Subject, partofspeech, cue, target,  Age, Age2, Age3, cue_phon, target_phon,  cue_code, target_code, phon_dist, CosSim, CosSim_cds) 

```

##### Wikipedia-based co-occurrence similarity

```{r echo=FALSE}

SemDist <- d_all %>%
  filter(!is.na(CosSim),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         ) %>%
  group_by(Age2) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 
  

#SemDist$Age3 = factor(SemDist$Age3, levels = c("Young", "Adult"))
SemDist$Age2 = factor(SemDist$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(SemDist, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1))+
  #geom_hline(yintercept = pair_chance$mean) + 
  ylab('mean semantic distance')
  
  #+ facet_grid( . ~ partofspeech) 

```

##### CHILDES-based co-occurrence similarity

```{r echo=FALSE}

SemDist <- d_all %>%
  filter(!is.na(CosSim_cds),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         ) %>%
  group_by(Age2) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 
  

#SemDist$Age3 = factor(SemDist$Age3, levels = c("Young", "Adult"))
SemDist$Age2 = factor(SemDist$Age2, levels = c("Younger", "Older", "Adult"))

ggplot(SemDist, aes(x=Age2, y=mean))+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1))+
  ylab('mean semantic distance')
  
  #+ facet_grid( . ~ partofspeech) 

```

For both models, I find a clear developmental trend in terms of the semantic proximity of the associations. 

One might wonder if this is related to the tension with phonetic similarity. That is, children's answers are less semantically similar because children also tend to give phonetically similar answers (which are not necessarily semantically related). If this is the case, we should expect a greater drop in semantic similarity for shorter phonetic distances. I will explore this in the next section.

#### Result 4: Correlation of Semantic  vs. Phonetic similarity


```{r include=FALSE}

Sem_histo <- d_all %>%
   filter(!is.na(CosSim_cds),
         phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
         )

ggplot(Sem_histo,  aes(CosSim_cds, fill=Age3))+geom_density(aes(y=..scaled..), alpha=0.2, adjust= 2) + theme(aspect.ratio = 0.7, axis.text=element_text(size=7, angle = 45)) +xlab("Cosine similarity") +ylab("Count")


```

For each value of the edit distance between the cue-targe relations, we compute the average semantic similarity.

For this analysis I collapsed "Older" and "Younger" into one age group "Young".


```{r include=FALSE}
##Correlation semantic phonology
phon_sem <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0,
           phon_dist < 8 # respones are rare and noisy beyound edit =7 
         )

ggplot(phon_sem, aes(x=phon_dist, y=CosSim))+
#ggplot(phon_sem_dist, aes(x=Phon_dist, y=CosSim))+
  geom_point()+
  geom_smooth(method = "lm")+
  #geom_smooth()+
  facet_grid(. ~ Age3)

#Correlatin test Adults (negative correlation)
adult <- subset(phon_sem, Age3=='Adult')
cor.test(adult$phon_dist, adult$CosSim)

#Correlatin test Children (positive correlation!!!)
children <- subset(phon_sem, Age3=='Young')
cor.test(children$phon_dist, children$CosSim)

```

##### Wikipedia-based co-occurrence similarity

```{r echo=FALSE}
 
phon_sem <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0,
           phon_dist < 8 # respones are rare and noisy beyound edit =7 
         ) %>%
  group_by(Age3,phon_dist) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 

phon_sem$phon_dist <- as.character(phon_sem$phon_dist)

phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0,
           phon_dist < 8 # respones are rare and noisy beyound edit =7 
         ) %>%
  group_by(Age3) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) %>%
  mutate(phon_dist = "all") %>%
  bind_rows(phon_sem)

phon_sem_ave$phon_dist = factor(phon_sem_ave$phon_dist, levels = c("all", "1", "2", "3","4", "5", "6","7" ))

ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  geom_hline(yintercept = pair_chance$mean, linetype=2) + 
  annotate("text", 'all', pair_chance$mean , vjust = 1, label = "chance")+
  xlab('Phonetic distance')+ ylab('semantic similarity')
  #ylim(c(0,0.75))#+
  #facet_grid(. ~ Age3)

```

##### CHILDES-based co-occurrence similarity

```{r echo=FALSE}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 
phon_sem <- d_all %>%
   filter(!is.na(CosSim_cds),
          !is.na(phon_dist),
           phon_dist != 0 ,
           phon_dist < 8 # respones are rare and noisy beyound edit =7
         ) %>%
  group_by(Age3,phon_dist) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


phon_sem$phon_dist <- as.character(phon_sem$phon_dist)

phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim_cds),
          !is.na(phon_dist),
           phon_dist != 0,
           phon_dist < 8 # respones are rare and noisy beyound edit =7 
         ) %>%
  group_by(Age3) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) %>%
  mutate(phon_dist = "all") %>%
  bind_rows(phon_sem)

phon_sem_ave$phon_dist = factor(phon_sem_ave$phon_dist, levels = c("all", "1", "2", "3","4", "5", "6","7" ))

ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  geom_hline(yintercept = pair_chance_cds$mean, linetype=2) +
  xlab('Phonetic distance')+ ylab('semantic similarity')+
  
  annotate("text", 'all',pair_chance_cds$mean , vjust = 1, label = "chance")
  #ylim(c(0,0.75))#+
  #facet_grid(. ~ Age3)


```

I found that the semantic similarity is, overall, higher in adults' associations across almost all values of phonetic distance. However, I also found that the highest discrepancy between children and adults was in the minimal pairs (i.e., phon_dist = 1). Children tend to provide targets that are phonetically minimal pairs with the cue, regardless of their semantic relatedness.  Indeed:

-As a first observation, the mean semantic similarity for minimal pairs was closer to chance than other values of phonetic distance (except for longer phonetic distances (> 5) where we also have lower statistical power).

-As a second analysis, we can compare the semantic similarity of the minimal pairs to the total average across all phonetic distances (far left of the graph). We see that, for children (but not adults), the semantic similarity of the minimal pairs is way lower than the average. (A more statistically sound comparison would require comparing the set of minimal pairs to the set of non-minimal pairs, but this is obviously going to give similar results). 

```{r include=FALSE}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 
phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  mutate(minimal = ifelse(phon_dist==1,"Minimal","noMinial")) %>%
  group_by(Age3, minimal) %>%
  summarise(mean = mean(CosSim),
            sd = sd(CosSim),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=minimal, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))+
  facet_grid(. ~ Age3)


```

```{r include=FALSE}
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of 

phon_sem_ave <- d_all %>%
   filter(!is.na(CosSim_cds),
          !is.na(phon_dist),
           phon_dist != 0 
         ) %>%
  mutate(minimal = ifelse(phon_dist==1,"Minimal","noMinial")) %>%
  group_by(Age3, minimal) %>%
  summarise(mean = mean(CosSim_cds),
            sd = sd(CosSim_cds),
                   n0 = n()) %>%
  mutate(se = sd / sqrt(n0),
         lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
  select(-sd, -n0, -se) 


  
ggplot(phon_sem_ave, aes(x=minimal, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = 0.1), size=0.3) +
  ylim(c(0,1))+
  facet_grid(. ~ Age3)


```


```{r include=FALSE}

hello <- d_all %>%
   filter(
          !is.na(phon_dist),
           phon_dist == 8 
         )
  
```

