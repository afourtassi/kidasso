geom_smooth()+
facet_grid(. ~ Age2)
#fit <- lm(CosSim~Phon_dist, data= subset(phon_sem_dist, Age2=='Older'))
#summary(fit)
##Here show the correlation between phonetinc and semantic distance
phon_sem_dist <- d_assoc_all %>%
filter(!is.na(Child_Word_phon),
!is.na(Child_Word),
Phon_dist != 0,
!is.na(CosSim)) #%>%
#  filter(!(Experimenter_Word %in% word_excl)) #%>%
#group_by(Age2, Phon_dist) %>%
#summarise(semSim = mean(CosSim))
ggplot(subset(phon_sem_dist, Phon_dist < 5), aes(x=Phon_dist, y=CosSim))+
#ggplot(phon_sem_dist, aes(x=Phon_dist, y=CosSim))+
geom_point()+
#geom_smooth(method = "lm")+
geom_smooth()+
facet_grid(. ~ Age2)
#fit <- lm(CosSim~Phon_dist, data= subset(phon_sem_dist, Age2=='Older'))
#summary(fit)
library(purrr)
library(readr)
library(ggplot2)
library(langcog)
library(purrr)
library(readr)
library(ggplot2)
#library(langcog)
library(boot)
library(dplyr)
library(tidyr)
library(wordbankr)
library(directlabels)
library(stringr)
library(lmtest)
library(rwebppl)
library(purrr)
library(readr)
library(ggplot2)
#library(langcog)
library(boot)
library(dplyr)
library(tidyr)
library(wordbankr)
library(directlabels)
library(stringr)
library(lmtest)
library(jsonlite)
library(nlme)
library(feather)
library(broom)
library(HDInterval)
library(BBmisc)
library(wordVectors)
library(magrittr)
library(lme4)
d <- read_delim("data/associations.txt", delim = "\t") %>%
mutate(Age3 = ifelse(Age2 == 'Older' | Age2 == 'Younger' , 'Young', 'Adult'))
#Throughout the analyses I collapse Younger and Older when there is no difference between their measures
dict <- read_delim("data/cmu_dict.txt", delim = ",")
###Function to prepocess the Glove vectos  (by Taylor Van Anne)
proc_pretrained_vec <- function(p_vec) {
# initialize space for values and the names of each word in vocab
vals <- vector(mode = "list", length(p_vec))
names <- character(length(p_vec))
# loop through to gather values and names of each word
for(i in 1:length(p_vec)) {
if(i %% 1000 == 0) {print(i)}
this_vec <- p_vec[i]
this_vec_unlisted <- unlist(strsplit(this_vec, " "))
this_vec_values <- as.numeric(this_vec_unlisted[-1])  # this needs testing, does it become numeric?
this_vec_name <- this_vec_unlisted[1]
vals[[i]] <- this_vec_values
names[[i]] <- this_vec_name
}
# convert lists to data.frame and attach the names
glove <- data.frame(vals)
names(glove) <- names
return(glove)
}
#glove.300 <- proc_pretrained_vec(g6b_300)
#convert to a  more appropriate foramt
#glove.matrix <- t(glove.300)
#This function is used to recode symbols in cmu phonetic dictionary: we want each symbol to be unique character. This is the right input for the Edit distance function (written by Megumi Sano)
characters <- read_delim("data/symbolsToChar.txt", delim = "\t", col_names = FALSE)
phones <- read_delim("data/cmu_symbols.txt", delim = "\t", col_names = FALSE)
phon_dict <- bind_cols(phones, characters)
#Converts string to its representation using the symbols
convert <- function(str, combined) {
if (typeof(str) != "character" && class(str) != "factor")
stop(sprintf("Illegal data type: %s", typeof(str)))
if (class(str) == "factor")
str = as.character(str)
if (length(str) == 0)
return(integer(0))
splitstring = strsplit(str, split=" ")
result_string = ""
for (i in splitstring) {
for (j in i) {
filtered <- filter(combined, X1 == j)
converted <- select(filtered, X11)
result_string <- paste(result_string, converted)
result_string <- gsub("[[:space:]]", "", result_string)
}
}
return(result_string)
}
#Number of participants by age group
n_subjects <- d %>%
group_by(Age2, Subject) %>%
summarise(n_words=n()) %>%
group_by(Age2) %>%
summarise(N_subjects=n())
n_subjects
#This function is used to recode symbols in cmu phonetic dictionary: we want each symbol to be unique character. This is the right input for the Edit distance function (written by Megumi Sano)
characters <- read_delim("data/symbolsToChar.txt", delim = "\t", col_names = FALSE)
phones <- read_delim("data/cmu_symbols.txt", delim = "\t", col_names = FALSE)
phon_dict <- bind_cols(phones, characters)
#Converts string to its representation using the symbols
convert <- function(str, combined) {
if (typeof(str) != "character" && class(str) != "factor")
stop(sprintf("Illegal data type: %s", typeof(str)))
if (class(str) == "factor")
str = as.character(str)
if (length(str) == 0)
return(integer(0))
splitstring = strsplit(str, split=" ")
result_string = ""
for (i in splitstring) {
for (j in i) {
filtered <- filter(combined, X1 == j)
converted <- select(filtered, X11)
result_string <- paste(result_string, converted)
result_string <- gsub("[[:space:]]", "", result_string)
}
}
return(result_string)
}
#Number of participants by age group
n_subjects <- d %>%
group_by(Age2, Subject) %>%
summarise(n_words=n()) %>%
group_by(Age2) %>%
summarise(N_subjects=n())
n_subjects
#Number of words by age group, and average subjects by word
n_words <- d %>%
group_by(Age2, Experimenter_Word) %>%
summarise(n_subj=n()) %>%
group_by(Age2) %>%
summarise(N_cues=n(),
Ave_subjects = mean(n_subj))
n_words
#Age groups
age_groups <- d %>%
distinct(Age2, Age)
age_groups[order(age_groups$Age),]
d_paradig <- d %>%
group_by(Age2) %>%
summarise(mean = mean(Para),
sd = sd(Para),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)
d_paradig$Age2 = factor(d_paradig$Age2, levels = c("Younger", "Older", "Adult"))
ggplot(d_paradig, aes(x=Age2, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = .1)) +
ylab('mean praadigmatic answers')
#Definition of the entropy
#-sum(p*log_2(p))
#Normalized entropy (what we use here)
#-sum(p*log_2(p))/log_2(n_sample)
#Here I need a way to keep the distribution and add mean and CI only in the plot, so as to
d_entropy <- d %>%
group_by(Experimenter_Word, Age2, Child_Word) %>%
summarise(n=n()) %>%
mutate(p = n/sum(n)) %>%
group_by(Experimenter_Word, Age2) %>%
summarise(entropy = -sum(p*log2(p)),
total = n()) %>%
mutate(entropy_norm = entropy/log2(total)) %>%# Normalized entropy
filter(!is.na(entropy_norm)) %>%
group_by(Age2) %>%
summarise(mean = mean(entropy_norm),
sd = sd(entropy_norm),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se)
#d_entropy$Age3 = factor(d_entropy$Age3, levels = c("Young", "Adult"))
d_entropy$Age2 = factor(d_entropy$Age2, levels = c("Younger", "Older", "Adult"))
ggplot(d_entropy, aes(x=Age2, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = .1))+
ylab('mean normalized entropy')
#+facet_grid( . ~ partofspeech)
# Do some statistical tests?
#We comapre: how the entropy is distributed for each cue across age groups
##Model predicts entropy (response) by Age agroup (predictor)
#fit_entrop <- lmer(entropy_norm ~ Age3 + (1|Experimenter_Word), data= d_entropy)
#confint(fit_entrop)
#Associations have the form: Cue -> Target
#Here we compute the phonetic proximity between the cue and the target
#Phonetic transcripton for Cues
Cues <- d %>%
select(Experimenter_Word) %>%
distinct() %>%
mutate(Word = toupper(Experimenter_Word)) %>%
left_join(dict) %>%
select(-Word) %>%
rename(cue_phon = Phonetic)
#Phonetic transcripton for Targets
Targets <- d %>%
select(Child_Word) %>%
distinct() %>%
mutate(Word = toupper(Child_Word)) %>%
left_join(dict) %>%
select(-Word) %>%
rename(target_phon = Phonetic)
#Here Uncomment to do the
#Include in the originial dataset
#d_phon <- d %>%
#  left_join(Cues) %>%
#  left_join(Targets) %>%
#  rowwise() %>%
#  mutate( #herer we recode the phonetic transcription to be able to use it with edit distance built in R
#       cue_code = convert(cue_phon, phon_dict),
#    target_code = ifelse(!is.na(target_phon), convert(target_phon, phon_dict), NA) #make an if statement
#    ) %>%
#  mutate(phon_dist =  ifelse(!is.na(target_code), adist(cue_code, target_code), NA))
#
#feather::write_feather(d_phon, "derived/d_phon.feather")
d_phon <- feather::read_feather("derived/d_phon.feather") %>%
dplyr::rename(cue = Experimenter_Word,
target = Child_Word)
#compute average of phon distance
PhonDist <- d_phon %>%
filter(!is.na(phon_dist),
phon_dist != 0 #We  eliminate words where children just repeat the cue (e.g., apple -> apple)
) %>%
group_by(Age2) %>%
summarise(mean = mean(phon_dist),
sd = sd(phon_dist),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
#PhonDist$Age3 = factor(PhonDist$Age3, levels = c("Young", "Adult"))
PhonDist$Age2 = factor(PhonDist$Age2, levels = c("Younger", "Older", "Adult"))
ggplot(PhonDist, aes(x=Age2, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = .1)) +
ylab('mean Phonetic edit distance')
#fit_phon <- lmer(phon_dist ~ Age3 + (Age3|cue) + (1|Subject) , data= PhonDist)
#confint(fit_phon)
#Historams
Phon_histo <- d_phon %>%
filter(!is.na(phon_dist),
phon_dist != 0
) %>%
group_by(Age3, phon_dist) %>%
summarise(n0 = n()) %>%
mutate(p = n0/sum(n0))
#ggplot(Phon_histo, aes(x=phon_dist, y=p))+
#  geom_col()+
#  facet_grid(. ~ Age3)
#How to quantify the rate of minimal pairs?
#E.g., for each cue, what's the probability that the target will be a minimal pair
#This means, for a given cue, what is the percentage of subjets of gave a minimal pair
miniDist <- d_phon %>%
filter(!is.na(phon_dist),
phon_dist != 0
) %>%
group_by(Age3, cue) %>% #Get a measure for each word, across subjects
summarise(n = n(),
dist1 = sum(phon_dist==1)/n(),
dist2 = sum(phon_dist==2)/n(),
dist3 = sum(phon_dist==3)/n(),
dist4 = sum(phon_dist==4)/n(),
dist5 = sum(phon_dist==5)/n(),
dist6 = sum(phon_dist==6)/n(),
dist7 = sum(phon_dist==7)/n(),
dist8 = sum(phon_dist==8)/n(),
dist9 = sum(phon_dist==9)/n(),
dist10 = sum(phon_dist==10)/n(),
dist11 = sum(phon_dist==11)/n()
) %>%
gather(phon_dist, prob, dist1:dist11) %>%
group_by(Age3, phon_dist) %>%
summarise(mean = mean(prob),
sd = sd(prob),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
miniDist$phon_dist = factor(miniDist$phon_dist, levels = c("dist1", "dist2", "dist3", "dist4", "dist5", "dist6", "dist7", "dist8", "dist9", "dist10", "dist11"))
ggplot(miniDist, aes(x=phon_dist, y=mean, fill=Age3))+
geom_line(aes(x=phon_dist, y=mean))+
geom_col(position = "dodge")+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = 1), size=0.3)
#Uncomment the following to preprocess the semantic distance
##Uncomment the following code to re-run the Word2Vec model
#model = train_word2vec("data/corpus.txt",
#                            output="corpus.bin", threads = 4,
#                             vectors = 100, window=20, cbow=1, min_count = 10, force= TRUE)
#Read the model
#model_cds = read.vectors("derived/corpus.bin")
#model_google = read.vectors("derived/freebase.bin")
#model <- as.VectorSpaceModel(example)
#
#cue_words <- (d %>%
#  distinct(Experimenter_Word))$Experimenter_Word
#target_words <- (d %>%
#  distinct(Child_Word))$Child_Word
#Glove vectors
#model_cue <- model[[which(rownames(model) %in% cue_words), average=FALSE]]
#model_target <- model[[which(rownames(model) %in% target_words), average=FALSE]]
#cosSim <- cosineSimilarity(model_cue, model_target)
#pairs <- na.omit(data.frame(as.table(cosSim))) %>%
#  rename(Experimenter_Word=Var1,
#         Child_Word=Var2,
#         CosSim=Freq)
#feather::write_feather(pairs, "derived/CosSim_glov.feather")
pairs <- feather::read_feather("derived/CosSim_glov.feather")%>%
dplyr::rename(cue = Experimenter_Word,
target = Child_Word)
#CDS
#model_cue_cds <- model_cds[[which(rownames(model_cds) %in% cue_words), average=FALSE]]
#model_target_cds <- model_cds[[which(rownames(model_cds) %in% target_words), average=FALSE]]
#cosSim_cds <- cosineSimilarity(model_cue_cds, model_target_cds)
#pairs_cds <- na.omit(data.frame(as.table(cosSim_cds))) %>%
#  rename(Experimenter_Word=Var1,
#         Child_Word=Var2,
#        CosSim_cds=Freq)
#feather::write_feather(pairs_cds, "derived/CosSim_cds.feather")
pairs_cds <- feather::read_feather("derived/CosSim_cds.feather") %>%
dplyr::rename(cue = Experimenter_Word,
target = Child_Word)
d_phon_sem <- d_phon %>%
left_join(pairs) %>%
left_join(pairs_cds)
#Basline (chance) in the semantic distance
pair_chance <- pairs %>%
summarise(mean = mean(CosSim),
sd = sd(CosSim),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
#Baseline (chance) semantic distance CDS
pair_chance_cds <- pairs_cds %>%
summarise(mean = mean(CosSim_cds),
sd = sd(CosSim_cds),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
#Here select the variables that are useful to later analyses
d_all <- d_phon_sem %>%
select(Subject, partofspeech, cue, target,  Age, Age2, Age3, cue_phon, target_phon,  cue_code, target_code, phon_dist, CosSim, CosSim_cds)
SemDist <- d_all %>%
filter(!is.na(CosSim),
phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
) %>%
group_by(Age2) %>%
summarise(mean = mean(CosSim),
sd = sd(CosSim),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
#SemDist$Age3 = factor(SemDist$Age3, levels = c("Young", "Adult"))
SemDist$Age2 = factor(SemDist$Age2, levels = c("Younger", "Older", "Adult"))
ggplot(SemDist, aes(x=Age2, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = .1))+
#geom_hline(yintercept = pair_chance$mean) +
ylab('mean semantic distance')
#+ facet_grid( . ~ partofspeech)
SemDist <- d_all %>%
filter(!is.na(CosSim_cds),
phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
) %>%
group_by(Age2) %>%
summarise(mean = mean(CosSim_cds),
sd = sd(CosSim_cds),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
#SemDist$Age3 = factor(SemDist$Age3, levels = c("Young", "Adult"))
SemDist$Age2 = factor(SemDist$Age2, levels = c("Younger", "Older", "Adult"))
ggplot(SemDist, aes(x=Age2, y=mean))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = .1))+
ylab('mean semantic distance')
#+ facet_grid( . ~ partofspeech)
Sem_histo <- d_all %>%
filter(!is.na(CosSim_cds),
phon_dist != 0 #We also eliminate words where children just repeat the cue (e.g., apple -> apple)
)
ggplot(Sem_histo,  aes(CosSim_cds, fill=Age3))+geom_density(aes(y=..scaled..), alpha=0.2, adjust= 2) + theme(aspect.ratio = 0.7, axis.text=element_text(size=7, angle = 45)) +xlab("Cosine similarity") +ylab("Count")
##Correlation semantic phonology
phon_sem <- d_all %>%
filter(!is.na(CosSim),
!is.na(phon_dist),
phon_dist != 0,
phon_dist < 8 # respones are rare and noisy beyound edit =7
)
ggplot(phon_sem, aes(x=phon_dist, y=CosSim))+
#ggplot(phon_sem_dist, aes(x=Phon_dist, y=CosSim))+
geom_point()+
geom_smooth(method = "lm")+
#geom_smooth()+
facet_grid(. ~ Age3)
#Correlatin test Adults (negative correlation)
adult <- subset(phon_sem, Age3=='Adult')
cor.test(adult$phon_dist, adult$CosSim)
#Correlatin test Children (positive correlation!!!)
children <- subset(phon_sem, Age3=='Young')
cor.test(children$phon_dist, children$CosSim)
phon_sem <- d_all %>%
filter(!is.na(CosSim),
!is.na(phon_dist),
phon_dist != 0,
phon_dist < 8 # respones are rare and noisy beyound edit =7
) %>%
group_by(Age3,phon_dist) %>%
summarise(mean = mean(CosSim),
sd = sd(CosSim),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
phon_sem$phon_dist <- as.character(phon_sem$phon_dist)
phon_sem_ave <- d_all %>%
filter(!is.na(CosSim),
!is.na(phon_dist),
phon_dist != 0,
phon_dist < 8 # respones are rare and noisy beyound edit =7
) %>%
group_by(Age3) %>%
summarise(mean = mean(CosSim),
sd = sd(CosSim),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se) %>%
mutate(phon_dist = "all") %>%
bind_rows(phon_sem)
phon_sem_ave$phon_dist = factor(phon_sem_ave$phon_dist, levels = c("all", "1", "2", "3","4", "5", "6","7" ))
ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = 0.1), size=0.3) +
geom_hline(yintercept = pair_chance$mean, linetype=2) +
annotate("text", 'all', pair_chance$mean , vjust = 1, label = "chance")+
xlab('Phonetic distance')+ ylab('semantic similarity')
#ylim(c(0,0.75))#+
#facet_grid(. ~ Age3)
#Analysis: compute the mean (mu), and the standard deviation (sd) for all values of phonetic distances,  then compute the , z-score of
phon_sem <- d_all %>%
filter(!is.na(CosSim_cds),
!is.na(phon_dist),
phon_dist != 0 ,
phon_dist < 8 # respones are rare and noisy beyound edit =7
) %>%
group_by(Age3,phon_dist) %>%
summarise(mean = mean(CosSim_cds),
sd = sd(CosSim_cds),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se)
phon_sem$phon_dist <- as.character(phon_sem$phon_dist)
phon_sem_ave <- d_all %>%
filter(!is.na(CosSim_cds),
!is.na(phon_dist),
phon_dist != 0,
phon_dist < 8 # respones are rare and noisy beyound edit =7
) %>%
group_by(Age3) %>%
summarise(mean = mean(CosSim_cds),
sd = sd(CosSim_cds),
n0 = n()) %>%
mutate(se = sd / sqrt(n0),
lower = mean - qt(1 - (0.05 / 2), n0 - 1) * se,
upper = mean + qt(1 - (0.05 / 2), n0 - 1) * se) %>%
select(-sd, -n0, -se) %>%
mutate(phon_dist = "all") %>%
bind_rows(phon_sem)
phon_sem_ave$phon_dist = factor(phon_sem_ave$phon_dist, levels = c("all", "1", "2", "3","4", "5", "6","7" ))
ggplot(phon_sem_ave, aes(x=phon_dist, y=mean, col=Age3))+
geom_pointrange(aes(ymin = lower, ymax = upper),
position = position_dodge(width = 0.1), size=0.3) +
geom_hline(yintercept = pair_chance_cds$mean, linetype=2) +
xlab('Phonetic distance')+ ylab('semantic similarity')+
annotate("text", 'all',pair_chance_cds$mean , vjust = 1, label = "chance")
#ylim(c(0,0.75))#+
#facet_grid(. ~ Age3)
